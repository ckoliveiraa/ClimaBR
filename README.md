# ğŸŒ¤ï¸ ClimaBR

**ClimaBR** Ã© um pipeline de dados automatizado na AWS que coleta, processa e transforma informaÃ§Ãµes climÃ¡ticas provenientes da API pÃºblica [Visual Crossing](https://www.visualcrossing.com/), utilizando orquestraÃ§Ã£o via Step Functions e transformaÃ§Ã£o com dbt, com o objetivo de comparar os dados climÃ¡ticos entre as 27 capitais do Brasil.

## ğŸ“Œ VisÃ£o Geral

O projeto realiza diariamente a ingestÃ£o de dados climÃ¡ticos, salva no Amazon S3, cataloga com o Glue Crawler, executa consultas com Athena e transforma os dados com dbt em trÃªs camadas: **Staging**, **Intermediate** e **Marts**.

![Arquitetura do Projeto](./images/image.png)

## ğŸ› ï¸ Tecnologias Utilizadas

- **AWS Lambda** â€” coleta dados da API  
- **Amazon S3** â€” armazenamento dos dados brutos  
- **AWS Glue Crawler** â€” catalogaÃ§Ã£o automÃ¡tica  
- **Amazon Athena** â€” anÃ¡lise de dados com SQL  
- **AWS Step Functions** â€” orquestraÃ§Ã£o dos processos  
- **Amazon EventBridge** â€” agendamento diÃ¡rio  
- **AWS CodeBuild** â€” execuÃ§Ã£o automatizada do dbt  
- **dbt Core** â€” transformaÃ§Ã£o e modelagem dos dados  
- **GitHub** â€” versionamento do projeto dbt

<details>
  <summary>Ver cÃ³digo da funÃ§Ã£o Lambda</summary>

```python
import json
import os
import requests
import boto3
from datetime import datetime, timedelta

def lambda_handler(event, context):

    capitais = [
        "Rio Branco, BR", "MaceiÃ³, BR", "MacapÃ¡, BR", "Manaus, BR", "Salvador, BR",
        "Fortaleza, BR", "BrasÃ­lia, BR", "VitÃ³ria, BR", "GoiÃ¢nia, BR", "SÃ£o LuÃ­s, BR",
        "CuiabÃ¡, BR", "Campo Grande, BR", "Belo Horizonte, BR", "BelÃ©m, BR", "JoÃ£o Pessoa, BR",
        "Curitiba, BR", "Recife, BR", "Teresina, BR", "Rio de Janeiro, BR", "Natal, BR",
        "Porto Alegre, BR", "Porto Velho, BR", "Boa Vista, BR", "FlorianÃ³polis, BR", "SÃ£o Paulo, BR",
        "Aracaju, BR", "Palmas, BR"
    ]

    today = datetime.today().date()
    tomorrow = today + timedelta(days=1)

    start_date = today.strftime('%Y-%m-%d')
    end_date = tomorrow.strftime('%Y-%m-%d')
    partition_date = today.strftime('%Y-%m-%d')

    api_key = os.environ['VISUAL_CROSSING_API_KEY'] #Criar variÃ¡vel de ambiente
    bucket_name = os.environ['BUCKET_NAME'] #Criar variÃ¡vel de ambiente
    s3 = boto3.client('s3')

    for cidade in capitais:
        try:
            url = f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{cidade}/{start_date}/{end_date}'
            params = {
                'unitGroup': 'metric',
                'include': 'days',
                'key': api_key,
                'contentType': 'json'
            }

            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()

            timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
            cidade_formatada = cidade.replace(', ', '_').replace(' ', '_').lower()
            file_name = f"clima/date={partition_date}/{cidade_formatada}_{timestamp}.json"

            s3.put_object(Bucket=bucket_name, Key=file_name, Body=json.dumps(data))
            print(f"{cidade} salva em {file_name}")

        except Exception as e:
            print(f"Erro ao processar {cidade}: {e}")

    return {
        'statusCode': 200,
        'body': f'Processamento concluÃ­do para todas as capitais. Data de particionamento: {partition_date}'
    }
```

</details>

## âš™ï¸ Pipeline de ExecuÃ§Ã£o

1. **Agendamento DiÃ¡rio** (EventBridge)  
2. **Coleta de Dados** da API pela Lambda  
3. **Armazenamento no S3**  
4. **Crawler Glue** atualiza o catÃ¡logo  
5. **Athena** atualiza metadados  
6. **ExecuÃ§Ã£o dbt** com CodeBuild (run, test, docs)  
7. **Dados Modelados** nas camadas:
   - `staging`
   - `intermediate`
   - `marts`

## ğŸ“ Estrutura de DiretÃ³rios dbt

```
ClimaBR/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ stg_clima.sql                # Modelo raw dos dados da API
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ int_clima_diario.sql         # Enriquecimento dos dados climÃ¡ticos
â”‚   â””â”€â”€ marts/
â”‚       â”œâ”€â”€ fact_analise_mensal.sql      # Fato mensal de mÃ©tricas climÃ¡ticas
â”‚       â””â”€â”€ fact_desvio_previsao.sql     # Fato com desvio entre previsÃ£o e combinado
â”œâ”€â”€ macros/
â”œâ”€â”€ dbt_project.yml
â””â”€â”€ ...
```

## ğŸ§© Macros dbt

O projeto contÃ©m macros personalizadas que auxiliam na transformaÃ§Ã£o dos dados:

```
macros/
â”œâ”€â”€ classify_moon_phase.sql       # ClassificaÃ§Ã£o da fase da lua com base na data
â”œâ”€â”€ generate_schema_name.sql      # GeraÃ§Ã£o dinÃ¢mica de nomes de schema
â”œâ”€â”€ schema.yml                    # DefiniÃ§Ãµes e descriÃ§Ãµes das macros
```
OBS.: Maiores detalhes na documentaÃ§Ã£o dbt

## ğŸ‘¤ Autor

- **Carlos Oliveira**  
- Contato: [LinkedIn](https://www.linkedin.com/in/carlosoliveira2910/) | [GitHub](https://github.com/ckoliveiraa)
